---
title: "M3_W1_Finding_data_and_reading_files"
author: "Alexander Cormack"
date: "21 August 2022"
output: html_document
---



### Raw and processed data

"Data are values of qualitative or quantative varaibles, belongning to a set of items."

http://en.wikipedia.org/wiki/Data

Set of items: sometimes called the population; the set of objects you are interested in
Variable: a measurement or characteristic of an item
Qualitative: country of origin, sex, treatment
Quantitative: height, weight, blood pressure


Raw versus processed data

Raw data

* The orgiginal source of the data
* Often hard to use for data analyses
* Data analysis includes processing
* Raw data may only need to be processed once

http://en.wikipedia.org/wiki/Raw_data

Processed data

* Data that is ready for analysis
* processing can include merging, subsetting, transforming, etc.
* There may be standards for processing
* All steps should be recorded (absolutely critical!)

http://en.wikipedia.org/wiki/Computer_data_processing



### The components of tidy data

The four things you should have when you have finished going from a raw dataset to a processed dataset:

1. The raw data
2. A tidy dataset
3. A code book describing each variable and its values in the tidy dataset
4. An explicit and exact recipe you used to go from 1 -> 2, 3, i.e. report the exact steps you took (can be R scripts)

One - The raw data

* The strange binary file you measurement machine spits out
* The unformatted Excel file with 10 worksheets the company you contracted with sent you
* The complicated JSON data you got from scraping the Twitter API
* The hand-entered numbers you collected looking through your microscope

You know the raw data is in the right format if you:

a. ran no software on the data
b. did not manipulate any of the numbers in the data
c. you did not remove any data from the dataset
d. You did not summarise the data in any way

https://github.com/jtleek/datasharing


Two - The tidy data

a. Each variable you measure should be in one column
b. Each different observation of that variable should be in a different row
c. There should be one table for each "kind" of variable
d. If you have multiple tables, they should include a column in the table that allows them to be linked

Some other important tips

* Include a row at the top of each file with variable names
* Make variable names human readable - AgeAtDiagnosis rather than AgeDx
* In general data should be saved in one file per table

https://github.com/jtleek/datasharing


Three - The code book

a. Information about the variable (including units - in thousands or millions?) in the dataset not contained in the tidy data
b. Information about the summary choices you made (e.g. monthly revenue ... was mean or median taken?)
c. Information about the experimental study design you used (how did you collect the data, any tests you did)

Some other important tips

* A common format for this document is a Word/text file (a markdown file)
* There should be a section called "Study design" that has a thorough description of how you collected the data
* There must be a section called "Code book" that describes each variable and its units

https://github.com/jtleek/datasharing


Four - The instruction list

* Ideally a computer script (in R, but Python is ok too ...)
* The input for the script is the raw data
* The output is the processed, tidy data
* There are no parameters to the script

In some cases it will not be possible to script every step. In that case you should provide instructions like:

a. Step 1 - take the raw file, run version 3.1.2 of summarise software with parameters a = 1, b = 2, c = 3
b. Step 2 - run the software separately for each sample
c. Step 3 - take column three of outputfile.out for each sample and that is the corresponding row in teh output dataset

https://github.com/jtleek/datasharing



### Get/set your working directory

* A basic component of working with data is knowing your working directory
* The two main commands are getwd() and setwd()
* Be aware of relatve versus absolute paths:
    + Relative - setwd(".data"), setwd("../")
    + Absolute - setwd("Users/cormaal/data/")
* Important difference in Windows setwd("C:\\Users\\Andrew\\Downloads")



### Checking for and creating directories

You can use the command file.exists("directoryName") to check to see if the directory exists.

You can use the command dir.create("directoryName") to create a directory if it doesn't exist.

Here is an example of both commands together:

```{r}
if (!file.exists("data")) {
        dir.create("data")
}
```



### Download the file to load

We will be using football data to practice downloading files from the internet:

https://www.football-data.co.uk/data.php

```{r}
if (!file.exists("data")) {
        dir.create("data")
}
fileUrl <- "https://www.football-data.co.uk/mmz4281/2122/E0.csv"
download.file(fileUrl, destfile = "./data/premier_league.csv", method = "curl")
dateDownloaded <- date()
```



### Loading flat files - read.table()

* This is the main function for reading data into R
* Flexible and robust but requires more parameters
* Reads the data into RAM - big data can cause problems
* Important parameters file, header, sep, row.names, nrows
* Related: read.csv(), read.csv2()

Example: Baltimore camera data

```{r}
premier_league <- read.table("./data/premier_league.csv", sep = ",", header = TRUE)
head(premier_league, 3)
```



The command read.csv sets sep = "," and header = TRUE

```{r}
cameraData <- read.csv("./data/premier_league.csv")
head(premier_league, 3)
```



Some more important parameters:

* quote - you can tell R whether there are any quoted values quote = "" means no quotes
* na.strings - set the character that represents a missing value
* nrows - how many rows to read of the file (e.g. nrows =  10 reads 10 lines)
* skip - number of lines to skip before starting to read

The biggest trouble while reading flat files are quotation marks ` or " placed in data values - setting quote = "" often resolves this


For Excel files there are the commands read.xlsx(), read.xlsx2(), {xlsx package}

```{r}
library(xlsx)
premier_league <- read.xlsx("./data/premier_league.xlsx", sheetIndex = 1, header = TRUE)
head(premier_league, 3)
```



Reading specific rows and columns

```{r}
premier_league_subset <- read.xlsx("./data/premier_league.xlsx",sheetIndex = 1, colIndex = 2:3, rowIndex = 1:4)
premier_league_subset
```



### Reading XML files

Tags, elements and attributes

* Tags correspond to general labels
    + start tags <section>
    + end tags </section>
    + empty tags <line-break />

* Elements are specific examples of tags
    + <Greeting> Hello, world! </Greeting>

* Attribute are components of the lable
    + <step number = "3"> Connect A to B. </step>

http://en.wikipedia.org/wiki/XML



Reading the file into R (passing useInternal = TRUE gives all of the nodes inside the file):

```{r, message=FALSE, warning=FALSE}
library(XML)
fileUrl <- "./data/sample.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
```



```{r}
names(rootNode)
```



Directly access parts of the XML document:

```{r}
rootNode[[1]]
```



```{r}
rootNode[[1]][[1]]
```



XPath

A whole new language!!! But with just a few components you will be able to extract specific data from XML files.

* /node Top level nod
* // Node at any level
* node[@attr-name] Node with an attribute name
* node[@attr-name='bob'] NOde with attribute name attr-name = 'bob'


Use the command xpathSApply to loop through the xXML file and extract all of the company names

```{r}
xpathSApply(rootNode, "//CompanyName", xmlValue)
```



Now let's get all of the company addresses:

```{r}
xpathSApply(rootNode, "//Address", xmlValue)
```



### Handling JSON files

* Javascript Object Notation
* Lightweight data storage
* Common format for data from application programming interfaces (APIs)
* Similar structure to XML but different syntax/format
* Data stored as:
    + numbers (double)
    + strings (double quoted)
    + Boolean (true or false)
    + array (ordered, comma separated enclosed in square brackets [])
    + object (unordered, comma separated collection of key:value pairs in curly brackets {})

http://en.wikipedia.org/wiki/JSON



Reading data from JSON {jsonlite package}

```{r}
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData)
```



Nested objects in JSON:

```{r}
names(jsonData$owner)
```



```{r}
jsonData$owner$login
```


Writing dataframes to JSON:

```{r}
myjson <- toJSON(iris, pretty = TRUE)
cat(myjson)
```



Converting back to JSON:

```{r}
iris2 <- fromJSON(myjson)
head(iris2)
```



Further information: http://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/

