allTables <- dbListTables(hg19)
length(allTables)
allTables[1:5]
dbListFields(hg19, "affyU133Plus2")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(affyMis$misMatches)
affyMisSmall <- fetch(query, n = 10); dbClearResult(query)
dim(affyMisSmall)
dbDisconnect(hg19)
library(RMySQL)
en(list = lm())
en(list = ls())
rm(list = ls())
getwd()
setwd(".\data")
setwd("./data")
setwd("C:/Users/CORMAAL/Desktop/DSEF/DS_Specialisation/03_Getting_And_Cleaning_Data/data")
if (!file.exists("data")) {
dir.create("data")
}
if (!file.exists("data")) {
dir.create("data")
}
if (!file.exists("data")) {
dir.create("data")
}
fileUrl <- "https:/data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "cameras.csv", method = "curl")
dateDownloaded <- date()
if (!file.exists("data")) {
dir.create("data")
}
fileUrl <- "https://www.football-data.co.uk/mmz4281/2122/E0.csv"
download.file(fileUrl, destfile = "premier_league.csv", method = "curl")
dateDownloaded <- date()
if (!file.exists("data")) {
dir.create("data")
}
fileUrl <- "https://www.football-data.co.uk/mmz4281/2122/E0.csv"
download.file(fileUrl, destfile = "./data/premier_league.csv", method = "curl")
dateDownloaded <- date()
premier_league <- read.table(".data/premier_league.csv", sep = ",", header = TRUE)
premier_league <- read.table("premier_league.csv", sep = ",", header = TRUE)
getwd()
premier_league <- read.table("premier_league.csv", sep = ",", header = TRUE)
cameraData <- read.csv("premier_league.csv")
premier_league <- read.table(".data/premier_league.csv", sep = ",", header = TRUE)
premier_league <- read.table("premier_league.csv", sep = ",", header = TRUE)
head(premier_league)
premier_league <- read.table(".data\premier_league.csv", sep = ",", header = TRUE)
premier_league <- read.table(".data\\premier_league.csv", sep = ",", header = TRUE)
premier_league <- read.table("~/data/premier_league.csv", sep = ",", header = TRUE)
setwd("C:/Users/CORMAAL/Desktop/DSEF/DS_Specialisation/03_Getting_And_Cleaning_Data")
premier_league <- read.table("~/data/premier_league.csv", sep = ",", header = TRUE)
premier_league <- read.table(".data/premier_league.csv", sep = ",", header = TRUE)
getwd()
premier_league <- read.table("./data/premier_league.csv", sep = ",", header = TRUE)
head(premier_league)
premier_league <- read.table("./data/premier_league.csv", sep = ",", header = TRUE)
premier_league <- read.table("./data/premier_league.csv", sep = ",", header = TRUE)
premier_league <- read.table("./data/premier_league.csv", sep = ",", header = TRUE)
head(premier_league)
premier_league <- read.table("./data/premier_league.csv", sep = ",", header = TRUE)
head(premier_league, 3)
cameraData <- read.csv(".data/premier_league.csv")
cameraData <- read.csv("./data/premier_league.csv")
head(premier_league, 3)
library(xlsx)
premier_league <- read.xlsx(".data/premier_league.xlsx", sheetIndex = 1, header = TRUE)
library(xlsx)
premier_league <- read.xlsx("./data/premier_league.xlsx", sheetIndex = 1, header = TRUE)
head(premier_league, 3)
premier_league_subset <- read.xlsx("./data/premier_league.xlsx", colIndex = 2:3, rowIndex = 1:4)
premier_league_subset <- read.xlsx("./data/premier_league.xlsx",sheetIndex = 1, colIndex = 2:3, rowIndex = 1:4)
premier_league_subset
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
library(XML)
fileUrl <- "https://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
rm(list = ls())
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
library(XML)
libcurlVersion()
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
library(XML)
fileUrl <- "https://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
names(rootNode)
names(rootNode)
rootNode[[1]]
rootNode[[1]]
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
library(XML)
fileUrl <- "https://childplus.com/admin/sales/xmlresources/"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
library(XML)
fileUrl <- "http://childplus.com/admin/sales/xmlresources/"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
names(rootNode)
library(XML)
fileUrl <- "./data/sample.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
rootNode[[1]]
rootNode[[1]][[1]]
xpathSApply(rootNode, "//name", xmlValue)
xpathSApply(rootNode, "//Customer", xmlValue)
xpathSApply(rootNode, "//PostalCode", xmlValue)
xpathSApply(rootNode, "//Customer", xmlValue)
xpathSApply(rootNode, "//CompanyName", xmlValue)
xpathSApply(rootNode, "//Address", xmlValue)
fileUrl <- "https://www.espn.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,  useInternalNodes = TRUE)
scores <- xpathApply(doc, "li//[@class = 'score']", xmlValue)
fileUrl <- "https://www.espn.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,  useInternalNodes = TRUE)
scores <- xpathApply(doc, "//li[@class = 'score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class - 'team-name']", xmlValue)
scores
fileUrl <- "http://www.espn.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,  useInternalNodes = TRUE)
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,  useInternalNodes = TRUE)
fileUrl <- "https://www.espn.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,  useInternalNodes = TRUE)
scores <- xpathApply(doc, "//li[@class = 'score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class - 'team-name']", xmlValue)
scores
fileUrl <- "https://www.espn.com/nfl/"
doc <- htmlTreeParse(fileUrl,  useInternalNodes = TRUE)
scores <- xpathApply(doc, "//li[@class = 'score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class - 'team-name']", xmlValue)
scores
fileUrl <- "http://www.espn.com/nfl/"
doc <- htmlTreeParse(fileUrl,  useInternalNodes = TRUE)
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData)
names(jsonData$owner)
jsonData$owner$login
myjson <- toJSON(iris, pretty = TRUE)
cat(myjson)
iris2 <- fromJSON(myjson)
head(iris2)
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install()
BiocManager::install(c("rhdf5"))
library(rhdf5)
created = h5createFile("example.h5")
created
created = h5createGroup("example.h5", "foo")
created = h5createGroup("example.h5", "baa")
created = h5createGroup("example.h5", "foo/foobaa")
h5ls("example.h5")
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install()
BiocManager::install(c("rhdf5"))
library(rhdf5)
created = h5createFile("example.h5")
created
A = matrix(1:10, nr = 5, nc = 2)
h5write(A, "example.h5", "foo/A")
B = array(seq(0.1, 2.0, by = 0.1), dim = c(5, 2, 2))
attr(B, "scale") <- "liter"
h5write(B, "example.h5", "foo/foobaa/B")
h5ls("example.h5")
df = data.frame(1L:5L, seq(0, 1, length.out = 5),
c("ab", "cde", "fghi", "a", "s"), stringsAsFactors = FALSE)
h5write(df, "example.h5", "df")
readA = h5read("example.h5", "foo/A")
readB = h5read("example.h5", "foo/foobaa/B")
readdf = h5read("example.h5", "df")
readA
readB
readdf
readB
readdf
h5write(c(12, 13, 14), "example.h5", "foo/A2", index = list(1:3, 1))
h5write(c(12, 13, 14), "example.h5", "foo/A", index = list(1:3, 1))
h5read("example.h5", "foo/A")
h5read("example.h5", "foo/A", index = list(1:3, 1))
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install()
BiocManager::install(c("rhdf5"))
library(rhdf5)
created = h5createFile("example.h5")
created
created = h5createGroup("example.h5", "foo")
created = h5createGroup("example.h5", "baa")
created = h5createGroup("example.h5", "foo/foobaa")
h5ls("example.h5")
A = matrix(1:10, nr = 5, nc = 2)
h5write(A, "example.h5", "foo/A")
B = array(seq(0.1, 2.0, by = 0.1), dim = c(5, 2, 2))
attr(B, "scale") <- "liter"
h5write(B, "example.h5", "foo/foobaa/B")
h5ls("example.h5")
df = data.frame(1L:5L, seq(0, 1, length.out = 5),
c("ab", "cde", "fghi", "a", "s"), stringsAsFactors = FALSE)
h5write(df, "example.h5", "df")
h5ls("example.h5")
readA = h5read("example.h5", "foo/A")
readB = h5read("example.h5", "foo/foobaa/B")
readdf = h5read("example.h5", "df")
readA
readB
readdf
h5write(c(12, 13, 14), "example.h5", "foo/A", index = list(1:3, 1))
h5read("example.h5", "foo/A")
h5read("example.h5", "foo/A", index = list(1:3, 1))
rm(list = ls())
GS_connection = url("http://scholar.google.com/citations?user=HI-I60AAAAJhl=en")
htmlCode = readLines(GS_connection)
GS_connection = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(GS_connection)
close(GS_connection)
htmlCode
library(XML)
url <- "http://scholar.google.com/citations?user=HT-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes = T)
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes = T)
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes = T)
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes = TRUE)
install.packages("XML")
install.packages("XML")
GS_connection = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(GS_connection)
close(GS_connection)
htmlCode
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes = T)
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes = TRUE)
library(rvest)
library(xml2)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
doc <- read_html(url)
doc %>%
html_nodes("a[href^='http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en']") %>%
html_attr("href")
?Sys.chmod
Sys.chmod("./support/TWitter_API.txt", mode = "0400")
Sys.setenv(BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAAG4CgQEAAAAAQgl%2FJITBl3UioomXjmpyDp9uz%2Fo%3D6xEkwnQYPYtlSMBMrtB1ot6mr7etopxrkk2rB8rSY2TyKgcUCO")
install.packages("httr")
install.packages("jsonlite")
install.packages("dplyr")
require(httr)
require(jsonlite)
require(dplyr)
bearer_token <- Sys.getenv("$AAAAAAAAAAAAAAAAAAAAAG4CgQEAAAAAQgl%2FJITBl3UioomXjmpyDp9uz%2Fo%3D6xEkwnQYPYtlSMBMrtB1ot6mr7etopxrkk2rB8rSY2TyKgcUCO")
headers <- c(`Authorization` = sprintf('Bearer %s', bearer_token))
params <- list(`user.fields` = 'description',
`expansions` = 'pinned_tweet_id')
handle <- readline('$EU_IPO')
sprintf('https://api.twitter.com/2/users/by?usernames=%s', handle)
handle <- readline('$EU_IPO')
url_handle <-
sprintf('https://api.twitter.com/2/users/by?usernames=%s', handle)
response <-
httr::GET(url = url_handle,
httr::add_headers(.headers = headers),
query = params)
obj <- httr::content(response, as = "text")
print(obj)
handle <- readline('$@EU_IPO')
url_handle <- sprintf('https://api.twitter.com/2/users/by?usernames=%s', handle)
response <-
httr::GET(url = url_handle,
httr::add_headers(.headers = headers),
query = params)
obj <- httr::content(response, as = "text")
print(obj)
handle <- readline('EU_IPO')
url_handle <- sprintf('https://api.twitter.com/2/users/by?usernames=%s', handle)
response <-
httr::GET(url = url_handle,
httr::add_headers(.headers = headers),
query = params)
obj <- httr::content(response, as = "text")
print(obj)
json_data <- fromJSON(obj, flatten = TRUE) %>% as.data.frame
View(json_data)
final <-
sprintf(
"Handle: %s\nBio: %s\nPinned Tweet: %s",
json_data$data.username,
json_data$data.description,
json_data$includes.tweets.text
)
cat(final)
final
str(final)
bearer_token
bearer_token <- Sys.getenv("AAAAAAAAAAAAAAAAAAAAAG4CgQEAAAAAQgl%2FJITBl3UioomXjmpyDp9uz%2Fo%3D6xEkwnQYPYtlSMBMrtB1ot6mr7etopxrkk2rB8rSY2TyKgcUCO")
headers <- c(`Authorization` = sprintf('Bearer %s', bearer_token))
headers
params <- list(`user.fields` = 'description',
`expansions` = 'pinned_tweet_id')
handle <- readline('$USERNAME')
url_handle <-
sprintf('https://api.twitter.com/2/users/by?usernames=%s', handle)
handle <- readline('EU_IPO')
response <-
httr::GET(url = url_handle,
httr::add_headers(.headers = headers),
query = params)
obj <- httr::content(response, as = "text")
print(obj)
handle <- readline('EU_IPO')
url_handle <-   sprintf('https://api.twitter.com/2/users/by?usernames=%s', handle)
url_handle
response <-
httr::GET(url = url_handle,
httr::add_headers(.headers = headers),
query = params)
obj <- httr::content(response, as = "text")
print(obj)
library(rjson)
require(httr)
require(jsonlite)
require(dplyr)
library(purrr)
tweet_id <- "70650192"
url_handle <- glue::glue("https://api.twitter.com/2/tweets/{status_id}/liking_users", status_id = tweet_id)
tweet_id <- "70650192"
url_handle <- glue::glue("https://api.twitter.com/2/users/by?usernames=%s", status_id = tweet_id)
response <-
httr::GET(url = url_handle,
httr::add_headers(.headers = headers),
query = params)
obj <- httr::content(response, as = "text")
print(obj)
response <- httr::GET(url = url_handle,
httr::add_headers(.headers = headers))
# query = params)
obj <- httr::content(response, as = "text")
obj
str(obj)
json_data <- fromJSON(obj, flatten = TRUE) %>% as.data.frame
x <- rjson::fromJSON(obj)
tweet_id <- "70650192"
url_handle <- glue::glue("https://api.twitter.com/2/tweets/{status_id}/liking_users", status_id = tweet_id)
response <- httr::GET(url = url_handle,
httr::add_headers(.headers = headers))
# query = params)
obj <- httr::content(response, as = "text")
x <- rjson::fromJSON(obj)
x$data %>%
purrr::map_chr("username")
tweet_id <- "1394072770661822464"
url_handle <- glue::glue("https://api.twitter.com/2/tweets/{status_id}/liking_users", status_id = tweet_id)
response <- httr::GET(url = url_handle,
httr::add_headers(.headers = headers))
# query = params)
obj <- httr::content(response, as = "text")
x <- rjson::fromJSON(obj)
x$data %>%
purrr::map_chr("username")
install.packages("sqldf")
library(sqldf)
acs <- read.csv("./data/get_data_ss06pid.csv")
setwd("C:/Users/CORMAAL/Desktop/DSEF/DS_Specialisation/03_Getting_And_Cleaning_Data/code")
acs <- read.csv("./data/get_data_ss06pid.csv")
acs <- read.csv("./data/getdata_data_ss06pid.csv")
View(acs)
probWeights1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
View(probWeights1)
probWeights2 <- sqldf("select pwgtp1 and AGEP from acs where AGEP < 50")
View(probWeights2)
probweights3 <- sqldf("select * from acs where AGEP < 50 and pwgtp1")
View(probweights3)
probweights4 <- sqldf("select AGEP, pwgtp1 from acs where AGEP < 50")
View(probweights4)
pw1_less_than_50_years <- sqldf("select pwgtp1 from acs where AGEP < 50")
pw1_and_ages_less_than_50_years <- sqldf("select AGEP, pwgtp1 from acs where AGEP < 50")
x <- unique(acs$AGEP)
y <- sqldf("select distinct AGEP from acs")
z <- sqldf("select AGEP where unique from acs")
rm(list = ls())
cat("\014")  # ctrl+L
library(XML)
library(httr)
url <- "http://biostat.jhsph.edu/~jleek/contact.html"
html2 = GET(url)
content2 = content(html2, as = "text")
parsedHTML = htmlParse(content2, asText = TRUE)
xpathSApply(parsedHTML, "//title", xmlValue)
content2
parsedHTML
parsedHTML[10]
n10 <- '<link rel="stylesheet" href="images/PixelGreen.css" type="text/css">'
x <- nchar(n10)
x
install.packages("foreign")
library(foreign)
?Foreign
data <- read.fortran("./data/getdata_wksst8110.for")
?read.fortran
data <- read.fortran(file = "./data/getdata_wksst8110.for", "14X", "F2.1", "12X")
data <- read.fortran(file = "./data/getdata_wksst8110.for", "3X", "F2.1", "4X")
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(-14, 3, -12))
data
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(-14, 3, -12))
head(data)
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(-27, 4))
head(data, 10)
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(-28, 4))
head(data, 10)
answer <- sum(data[5:])
str(data)
answer <- sum(data[5:, ])
filtered_data <- data[5:, 1]
filtered_data <- data[5:1258, ]
head(filtered_data, 10)
answer <- sum(filtered_data)
str(filtered_data)
numeric_data <- as.numeric(filtered_data)
answer <- sum(numeric_data)
amswer
naswer
answer
parsedHTML
htmlLines <- readLines(url, n=100)
htmlLines[10]
tenth <- nchar(htmlLines[10])
twentieth <- nchar(htmlLines[20])
thirtieth <- nchar(htmlLines[30])
one_hundredth <- nchar(htmlLines[100])
answer5 <- sum(numeric_data)
answer4 <- c(tenth, twentieth, thirtieth, one_hundredth)
answer4
rm(list = ls())
p_unload(all)  # Remove all add-ons
dev.off()  # But only if there IS a plot
cat("\014")  # ctrl+L
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(9, -5, 4, 4, -5, 4, 4, -5, 4, 4, -5, 4, 4))
library(foreign)
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(9, -5, 4, 4, -5, 4, 4, -5, 4, 4, -5, 4, 4))
install.packages("foreign")
setwd("C:/Users/CORMAAL/Desktop/DSEF/DS_Specialisation/03_Getting_And_Cleaning_Data/code")
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(9, -5, 4, 4, -5, 4, 4, -5, 4, 4, -5, 4, 4))
head(data, 10)
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(9, -6, 4, 4, -5, 4, 4, -5, 4, 4, -5, 4, 4))
head(data, 10)
str(data)
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(-1, 9, -5, 4, 4, -5, 4, 4, -5, 4, 4, -5, 4, 4))
head(data, 10)
data_cleaned <- data[-c(1, 2), ]
head(data_cleaned)
colnames(df) <- c("Week", "SST", "SSTA", "SST", "SSTA", "SST", "SSTA", "SST", "SSTA")
colnames(data_cleaned) <- c("Week", "SST", "SSTA", "SST", "SSTA", "SST", "SSTA", "SST", "SSTA")
head(data_cleaned)
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(-1, 9, -5, 4, 4, -5, 4, 4, -5, 4, 4, -5, 4, 4))
colnames(data_cleaned) <- c("Week", "Nino1+2 (SST)", "Nino1+2 (SSTA)", "Nino3 (SST)", "Nino3 (SSTA)",
"Nino34 (SST)", "Nino34 (SSTA)", "Nino4 (SST)", "Nino4 (SSTA)")
data_cleaned <- data[-c(1:4, 2), ]
head(data_cleaned)
colnames(data) <- c("Week", "Nino1+2 (SST)", "Nino1+2 (SSTA)", "Nino3 (SST)", "Nino3 (SSTA)",
"Nino34 (SST)", "Nino34 (SSTA)", "Nino4 (SST)", "Nino4 (SSTA)")
data_cleaned <- data[-c(1:4, 2), ]
head(data_cleaned, 10)
library(dplyr)
data_numeric <- data_cleaned %>% mutate_at(c("Nino1+2 (SST)", "Nino1+2 (SSTA)", "Nino3 (SST)", "Nino3 (SSTA)",
"Nino34 (SST)", "Nino34 (SSTA)", "Nino4 (SST)", "Nino4 (SSTA)"), as.numeric)
str(data_numeric)
head(data_numeric, 10)
head(data_numeric, row.names = FALSE, 10)
print(data_numeric, row.names = FALSE)
data_numeric, row.names = FALSE
data_numeric
rownames(data_numeric) <- c()
head
head(data_numeric, 10)
data <- read.fwf("./data/getdata_wksst8110.for", widths = c(-1, 9, -5, 4, 4, -5, 4, 4, -5, 4, 4, -5, 4, 4))
colnames(data) <- c("Week", "Nino1+2 (SST)", "Nino1+2 (SSTA)", "Nino3 (SST)", "Nino3 (SSTA)",
"Nino34 (SST)", "Nino34 (SSTA)", "Nino4 (SST)", "Nino4 (SSTA)")
data_cleaned <- data[-c(1:4, 2), ]
row.names(data_cleaned) <- c()
data_numeric <- data_cleaned %>% mutate_at(c("Nino1+2 (SST)", "Nino1+2 (SSTA)", "Nino3 (SST)", "Nino3 (SSTA)",
"Nino34 (SST)", "Nino34 (SSTA)", "Nino4 (SST)", "Nino4 (SSTA)"), as.numeric)
head(data_numeric, 10)
rm(list = ls())
p_unload(all)  # Remove all add-ons
dev.off()  # But only if there IS a plot
cat("\014")  # ctrl+L
